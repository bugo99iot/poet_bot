{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KDTree\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phenomen woman pretti women wonder secret cute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>still rise may write histori bitter twist lie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>road taken two road diverg yellow wood sorri c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>forget want know one thing know look crystal m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dream hold fast dream dream die life bird hold...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  phenomen woman pretti women wonder secret cute...\n",
       "1  still rise may write histori bitter twist lie ...\n",
       "2  road taken two road diverg yellow wood sorri c...\n",
       "3  forget want know one thing know look crystal m...\n",
       "4  dream hold fast dream dream die life bird hold..."
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"clean_text_poems.csv\", header=None)\n",
    "df=df.fillna(\" \")\n",
    "df.columns = [\"text\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine\n",
      "                                                  text\n",
      "0    phenomen woman pretti women wonder secret cute...\n",
      "1    still rise may write histori bitter twist lie ...\n",
      "2    road taken two road diverg yellow wood sorri c...\n",
      "3    forget want know one thing know look crystal m...\n",
      "4    dream hold fast dream dream die life bird hold...\n",
      "5    annabel lee mani mani year ago kingdom sea mai...\n",
      "6    cage bird free bird leap back wind float downs...\n",
      "7    keep head lose blame trust men doubt make allo...\n",
      "8    stop wood snowi even whose wood think hous vil...\n",
      "9    love except love love except love go love love...\n",
      "10   dream within dream take kiss upon brow part th...\n",
      "11   televis import thing learn far children concer...\n",
      "12   fire ice say world end fire say tast desir hol...\n",
      "13   stand grave weep stand grave weep sleep thousa...\n",
      "14   invictu night cover black pit pole pole thank ...\n",
      "15   grew older long time almost forgotten front br...\n",
      "16   alon childhood hour other seen other saw could...\n",
      "17   go far go far even day know say day long wait ...\n",
      "18   go gentl good night go gentl good night old ag...\n",
      "19   warn old woman shall wear purpl red hat go sui...\n",
      "20   hope thing feather thing feathers— perch soul—...\n",
      "21   poison tree angri friend told wrath wrath angr...\n",
      "22   wander lone cloud daffodil wander lone cloud f...\n",
      "23   world stage world stage men women mere player ...\n",
      "24   let die youngman death let die youngman death ...\n",
      "25   mother son well son tell life ai crystal tack ...\n",
      "26   highwayman part one wind torrent dark among gu...\n",
      "27   captain captain captain captain fear trip done...\n",
      "28   ~ choos mountain~ low land call tempt answer o...\n",
      "29   smile rememb goldfish circl around around bowl...\n",
      "..                                                 ...\n",
      "466  blood `` true arab know catch fli hand father ...\n",
      "467  wake tomorrow give poem wake peac ’ make ’ mak...\n",
      "468  haiku tast tast rain —whi kneel               ...\n",
      "469  sonnet vii soon hath time subtl thief youth so...\n",
      "470  littl boy blue littl toy dog cover dust sturdi...\n",
      "471  old man tree old man tree horribl bore said ``...\n",
      "472  ann anni gone whose eye compar morn sun idid c...\n",
      "473  peac sweet peac dost thou dwell humbl crave le...\n",
      "474  time time slow wait swift fear long griev shor...\n",
      "475  dawn dawn wind rush drive night ’ chill westwa...\n",
      "476  winter truth dark go bird silent one day long ...\n",
      "477  reveng ay gaze upon hair gaze upon smile seem ...\n",
      "478  wait wait distrust everyth trust hour carri ev...\n",
      "479  come come memori born mortal mother hous mani ...\n",
      "480  taught live simpli taught live simpli wise loo...\n",
      "481  old life snow fell woke bluish mound soft hond...\n",
      "482  hurt hawk broken pillar wing jag clot shoulder...\n",
      "483  accus tend past accus tend past made sculpt ha...\n",
      "484  monet refus oper doctor say halo around street...\n",
      "485  hope heart `` delicta juventuti et ignorantiu ...\n",
      "486  awhil shadow vanish smile feel warmth touch co...\n",
      "487  call good marriag call good marriag one ever q...\n",
      "488  great valuabl gift christma day “ wow ’ christ...\n",
      "489  winter night snow snow whole world snow swept ...\n",
      "490  new poet find new poet like find new wildflow ...\n",
      "491  pattern walk daffodil blow bright blue squill ...\n",
      "492  haiku low yellow low yellow moon quiet lamplit...\n",
      "493  pari talk love ear get tear down drink one tal...\n",
      "494  jenni kiss jenni kiss met jump chair sat time ...\n",
      "495  walk cemeteri visit friend { author recommend ...\n",
      "\n",
      "[496 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_plusone = pd.DataFrame({\"text\" : []}, columns=[\"text\"])\n",
    "ps = PorterStemmer()\n",
    "def clean_string(row):\n",
    "    tokens = word_tokenize(row.lower())\n",
    "    filtered_tokens = [ps.stem(word) for word in tokens if word not in stopwords.words('english') and bool(re.search(\"[-0123456789>(</',;:!?.)]\", word))==False]\n",
    "    return filtered_tokens\n",
    "search = \"wine\"\n",
    "search = \" \".join(clean_string(search)).strip().replace(\"  \", \"\")\n",
    "print(search)\n",
    "df_plusone = df.append({\"text\":search}, ignore_index=True)\n",
    "print(df_plusone.iloc[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['across', 'add', 'age', 'ago', 'air', 'aliv', 'almost', 'alon', 'along', 'alreadi', 'also', 'alway', 'among', 'ancient', 'angel', 'angri', 'anoth', 'answer', 'arm', 'around', 'arriv', 'art', 'ask', 'asleep', 'autumn', 'awak', 'away', 'babi', 'back', 'bad', 'bare', 'bay', 'bear', 'beat', 'beauti', 'becom', 'bed', 'bee', 'begin', 'behind', 'believ', 'bell', 'bend', 'beneath', 'besid', 'best', 'better', 'beyond', 'big', 'bird', 'birth', 'bitter', 'black', 'bless', 'blind', 'blood', 'bloom', 'blow', 'blue', 'bodi', 'bone', 'book', 'born', 'bough', 'bound', 'bow', 'boy', 'brain', 'branch', 'brave', 'bread', 'break', 'breast', 'breath', 'bright', 'bring', 'broken', 'brother', 'brought', 'brow', 'brown', 'bud', 'burn', 'call', 'calm', 'came', 'car', 'care', 'carri', 'catch', 'caus', 'chair', 'chanc', 'chang', 'charm', 'cheek', 'cheer', 'child', 'children', 'choos', 'circl', 'citi', 'clean', 'clear', 'climb', 'close', 'cloud', 'coat', 'cold', 'color', 'come', 'cool', 'copyright', 'corner', 'could', 'count', 'countri', 'cours', 'cover', 'creatur', 'cri', 'cross', 'crowd', 'cruel', 'cup', 'danc', 'dare', 'dark', 'dawn', 'day', 'dead', 'dear', 'death', 'deck', 'deep', 'delight', 'depth', 'desert', 'desir', 'despair', 'die', 'dim', 'dirt', 'distanc', 'distant', 'dog', 'done', 'door', 'doubt', 'dream', 'drew', 'dri', 'drink', 'drive', 'drop', 'drunk', 'dust', 'ear', 'earth', 'eas', 'easi', 'eat', 'edg', 'either', 'els', 'empti', 'end', 'enjoy', 'enough', 'ere', 'etern', 'even', 'ever', 'everi', 'everyon', 'everyth', 'exist', 'eye', 'face', 'fade', 'fair', 'faith', 'fall', 'far', 'fast', 'fate', 'father', 'fear', 'feed', 'feel', 'feet', 'fell', 'felt', 'field', 'fight', 'fill', 'find', 'fine', 'finger', 'fire', 'first', 'fish', 'fit', 'five', 'flame', 'flesh', 'fli', 'flight', 'float', 'flood', 'floor', 'flow', 'flower', 'follow', 'font', 'forest', 'forget', 'form', 'forth', 'found', 'free', 'freedom', 'fresh', 'friend', 'full', 'game', 'garden', 'gather', 'gaze', 'gentl', 'get', 'ghost', 'gift', 'girl', 'give', 'given', 'glad', 'glass', 'gleam', 'glori', 'glow', 'go', 'god', 'goe', 'gold', 'golden', 'gone', 'good', 'got', 'grace', 'grain', 'grass', 'grave', 'great', 'green', 'grew', 'grey', 'grief', 'griev', 'grow', 'grown', 'hair', 'half', 'hand', 'happen', 'happi', 'hard', 'hate', 'haunt', 'head', 'health', 'hear', 'heard', 'heart', 'heat', 'heaven', 'heavi', 'height', 'held', 'hell', 'help', 'hide', 'high', 'hill', 'hold', 'home', 'hope', 'hors', 'hot', 'hour', 'hous', 'human', 'hundr', 'hung', 'hungri', 'hurt', 'husband', 'ice', 'innoc', 'insid', 'joy', 'june', 'keep', 'kept', 'kill', 'kind', 'king', 'kiss', 'knew', 'know', 'known', 'lack', 'land', 'last', 'late', 'laugh', 'laughter', 'lay', 'lead', 'learn', 'leav', 'left', 'leg', 'less', 'lesson', 'let', 'letter', 'lie', 'life', 'lift', 'light', 'like', 'line', 'lip', 'listen', 'lit', 'littl', 'live', 'lone', 'long', 'longer', 'look', 'lord', 'lose', 'lost', 'love', 'lover', 'low', 'made', 'make', 'man', 'mani', 'mark', 'may', 'mean', 'meet', 'melt', 'memori', 'men', 'met', 'midnight', 'might', 'mile', 'mind', 'mine', 'miss', 'moan', 'moment', 'moon', 'morn', 'mother', 'mountain', 'mourn', 'mouth', 'move', 'much', 'murmur', 'music', 'must', 'name', 'natur', 'near', 'need', 'neither', 'never', 'new', 'next', 'night', 'none', 'noth', 'offer', 'oh', 'old', 'one', 'open', 'order', 'pain', 'pale', 'palm', 'park', 'part', 'pass', 'passion', 'past', 'path', 'peac', 'peopl', 'perfect', 'perhap', 'person', 'pick', 'place', 'play', 'pleas', 'pleasur', 'poem', 'poet', 'poetri', 'poor', 'power', 'pray', 'present', 'publish', 'pull', 'pure', 'purpl', 'put', 'question', 'quick', 'quiet', 'quit', 'race', 'rain', 'rais', 'rather', 'ray', 'reach', 'read', 'real', 'realli', 'reason', 'red', 'remain', 'rememb', 'repli', 'rest', 'return', 'rich', 'right', 'rise', 'river', 'road', 'roar', 'rock', 'roll', 'room', 'rose', 'round', 'run', 'rush', 'sad', 'said', 'sand', 'sat', 'saw', 'say', 'school', 'sea', 'second', 'secret', 'see', 'seek', 'seem', 'seen', 'send', 'sens', 'set', 'seven', 'sever', 'shade', 'shadow', 'shake', 'shall', 'shame', 'shape', 'share', 'sharp', 'shine', 'shore', 'short', 'shoulder', 'shout', 'show', 'shut', 'side', 'sigh', 'sight', 'silenc', 'silent', 'silver', 'sin', 'sinc', 'sing', 'sink', 'sit', 'skin', 'sky', 'sleep', 'slow', 'small', 'smell', 'smile', 'smoke', 'snow', 'soft', 'softli', 'soldier', 'someon', 'someth', 'sometim', 'somewher', 'son', 'song', 'soon', 'sorrow', 'sought', 'soul', 'sound', 'space', 'speak', 'spent', 'spirit', 'spite', 'spread', 'spring', 'stand', 'star', 'stare', 'start', 'stay', 'step', 'still', 'stir', 'stone', 'stood', 'stop', 'storm', 'strain', 'strang', 'stranger', 'stream', 'street', 'stretch', 'strong', 'sudden', 'suddenli', 'summer', 'sun', 'sunset', 'sure', 'sweat', 'sweet', 'swing', 'take', 'taken', 'talk', 'tast', 'taught', 'tear', 'tell', 'ten', 'tender', 'text', 'thee', 'thi', 'thin', 'thine', 'thing', 'think', 'thou', 'though', 'thought', 'thousand', 'three', 'thu', 'tide', 'till', 'time', 'tini', 'tire', 'today', 'togeth', 'told', 'tongu', 'took', 'top', 'touch', 'town', 'translat', 'travel', 'tree', 'tri', 'troubl', 'true', 'truth', 'turn', 'two', 'understand', 'upon', 'us', 'use', 'vain', 'vision', 'voic', 'wait', 'wake', 'walk', 'wall', 'wander', 'want', 'war', 'warm', 'wash', 'watch', 'water', 'wave', 'way', 'wear', 'weari', 'weep', 'well', 'went', 'wet', 'wheel', 'whisper', 'white', 'whole', 'whose', 'wide', 'wife', 'wild', 'win', 'wind', 'window', 'wine', 'wing', 'winter', 'wise', 'wish', 'within', 'without', 'woman', 'women', 'wonder', 'wood', 'word', 'work', 'world', 'wors', 'worth', 'would', 'write', 'wrong', 'ye', 'year', 'yellow', 'yet', 'young', 'youth']\n",
      "  (0, 644)\t1\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=0.01,decode_error=\"ignore\")\n",
    "\n",
    "# Fit and transform the training data \n",
    "#you ise fit only for the training data, otherwise you change the vocabulary (https://stackoverflow.com/questions/38692520/what-is-the-difference-between-fit-transform-and-transform-in-sklearn-countvecto)\n",
    "count_train = count_vectorizer.fit_transform(df_plusone.iloc[:,0]) \n",
    "print(count_vectorizer.get_feature_names())\n",
    "print(count_train[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look: http://andrewgaidus.com/Finding_Related_Wikipedia_Articles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0]]\n",
      "[500 288 127]\n"
     ]
    }
   ],
   "source": [
    "matrix=count_train.todense()\n",
    "print(matrix[-1,:])\n",
    "kdt = KDTree(matrix, leaf_size=30, metric='euclidean')\n",
    "results = kdt.query(matrix[-1,:], k=3, return_distance=False)  \n",
    "results = results[0]\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_poems = pd.read_csv(\"poems_collection.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey Father Death, I'm flying home\n",
      "Hey poor man, you're all alone\n",
      "Hey old daddy, I know where I'm going\n",
      "\n",
      "Father Death, Don't cry any more\n",
      "Mama's there, underneath the floor\n",
      "Brother Death, please mind the store\n",
      "\n",
      "Old Aunty Death Don't hide your bones\n",
      "Old Uncle Death I hear your groans\n",
      "O Sister Death how sweet your moans\n",
      "\n",
      "O Children Deaths go breathe your breaths\n",
      "Sobbing breasts'll ease your Deaths\n",
      "Pain is gone, tears take the rest\n",
      "\n",
      "Genius Death your art is done\n",
      "Lover Death your body's gone\n",
      "Father Death I'm coming home\n",
      "\n",
      "Guru Death your words are true\n",
      "Teacher Death I do thank you\n",
      "For inspiring me to sing this Blues\n",
      "\n",
      "Buddha Death, I wake with you\n",
      "Dharma Death, your mind is new\n",
      "Sangha Death, we'll work it through\n",
      "\n",
      "Suffering is what was born\n",
      "Ignorance made me forlorn\n",
      "Tearful truths I cannot scorn\n",
      "\n",
      "Father Breath once more farewell\n",
      "Birth you gave was no thing ill\n",
      "My heart is still, as time will tell. \r\n",
      "            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_poems.iloc[results[1],1].replace(\"<br/>\",\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.]]\n",
      "[500  94 136]\n"
     ]
    }
   ],
   "source": [
    "Tfid_vectorizer = TfidfVectorizer(min_df=0.01,decode_error=\"ignore\")\n",
    "Tfid_count_train = Tfid_vectorizer.fit_transform(df_plusone.iloc[:,0]) \n",
    "matrix=Tfid_count_train.todense()\n",
    "print(matrix[-1,:])\n",
    "kdt = KDTree(matrix, leaf_size=30, metric='euclidean')\n",
    "results = kdt.query(matrix[-1,:], k=3, return_distance=False)  \n",
    "results = results[0]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Flanders fields the poppies blow\n",
      "Between the crosses, row on row,\n",
      "That mark our place; and in the sky\n",
      "The larks, still bravely singing, fly\n",
      "Scarce heard amid the guns below.\n",
      "\n",
      "We are the Dead. Short days ago\n",
      "We lived, felt dawn, saw sunset glow,\n",
      "Loved and were loved, and now we lie,\n",
      "In Flanders fields.\n",
      "\n",
      "Take up our quarrel with the foe:\n",
      "To you from failing hands we throw\n",
      "The torch; be yours to hold it high.\n",
      "If ye break faith with us who die\n",
      "We shall not sleep, though poppies grow\n",
      "In Flanders fields. \r\n",
      "            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_poems.iloc[results[1],1].replace(\"<br/>\",\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
